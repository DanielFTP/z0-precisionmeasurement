{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Optimize lepton selection\n",
    "\n",
    "* First, print the distributions of the relevant variables for *all* the Monte Carlo samples (i.e. all the *channels* of the $Z$-boson decay to be studied). Which variables are these? Give sensible ranges to include all the events in the samples (both MC and OPAL data) \n",
    "* Do the same for **one** of the OPAL data samples (your lab assistant will decide which one you choose).\n",
    "* Describe the results.\n",
    "* Optimize the object selection by applying cuts. Make a strategy on how to proceed to find the optimal selection. which information do you need? in thin the\n",
    "* Determine the efficiency and the amount of background for each $Z$ decay channel. Use the simulated events $e^+e^-$, $\\mu^+\\mu^-$, $\\tau^+\\tau^-$ and hadrons ($qq$). Represent the result in a matrix form and think carefully about how you have to correct the measured rates. Don't forget to calculate the errors!\n",
    "* How do we estimate the statistical fluctuations per bin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download missing libraries\n",
    "Comment in the following two lines in case some of the libraries cannot be imported. Please restart the kernel after download+upgrade has successfully finished.\n",
    "\n",
    "**Please comment in these lines when the libraries cannot be imported below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download libraries\n",
    "#%pip install uproot \n",
    "#%pip install awkward \n",
    "#%pip install mplhep \n",
    "#%pip install numpy \n",
    "#%pip install matplotlib \n",
    "#%pip install scipy\n",
    "\n",
    "### Upgrade libraries to latest version\n",
    "#%pip install uproot awkward mplhep numpy matplotlib scipy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import mplhep\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import factorial\n",
    "from scipy.stats import poisson\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final version\n",
    "#Defining data path\n",
    "path_data = '/Users/sayed/OneDrive/Dokumente/GitHub/z0-precisionmeasurement/'\n",
    "\n",
    "#fit functions\n",
    "\n",
    "def lin(x,a,b):\n",
    "    return a*x+b\n",
    "def f(x,a,b):\n",
    "    return a*(1+x**2)+b/(1-x)**2\n",
    "def gauss(x, A, mu, sigma):\n",
    "    return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "def breit_wigner(s,Mz,gamma_fe, gamma_z):\n",
    "    s=s**2\n",
    "    return 12*np.pi/Mz**2*(s*gamma_fe)/((s-Mz**2)**2+(s**2*gamma_z**2/Mz**2))\n",
    "def fit_function(k, lamb):\n",
    "    '''poisson function, parameter lamb is the fit parameter'''\n",
    "    return poisson.pmf(k, lamb)\n",
    "################################################################################\n",
    "# testing functions\n",
    "\n",
    "\n",
    "#the following function describes in which sigma interval of the literature values\n",
    "# the measured or calculated values is \n",
    "# values between 0 and 3 indicate compatibility of the values whereby a value of 3 is borderline\n",
    "def ttest(x, sx, y, sy = 0):\n",
    "    return np.abs(x - y)/np.sqrt(sx**2 + sy**2)\n",
    "\n",
    "\n",
    "def redchi2(data,fit,sigma,n_param=2):\n",
    "    return np.sum((((data-fit)/sigma))**2) / (len(data)-n_param)\n",
    "\n",
    "############################################################################\n",
    "\n",
    "#reading data function\n",
    "\n",
    "def CSV(file):\n",
    "    rows = []\n",
    "    with open(file, 'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "    mean_energy=np.array(list(zip(*rows[1:]))[0], dtype=float)#*increment+start\n",
    "    lumi=np.array(list(zip(*rows[1:]))[1], dtype=float)\n",
    "    stat=np.array(list(zip(*rows[1:]))[2], dtype=float)\n",
    "    sys=np.array(list(zip(*rows[1:]))[3], dtype=float)\n",
    "    alll=np.array(list(zip(*rows[1:]))[4], dtype=float)\n",
    "    return mean_energy, lumi,stat, sys, alll\n",
    "\n",
    "###################################################################\n",
    "#particle counting functions\n",
    "\n",
    "def count_particles(p,N,E,HE): #for Grope data\n",
    "    num_had=0\n",
    "    num_muon=0\n",
    "    num_e=0\n",
    "    num_tau=0\n",
    "    for i in range(0,len(N)):\n",
    "        if N[i]>=10 and E[i]<=60 and E[i]>=20 and HE[i]>=5 and p[i]<=100:\n",
    "            num_had+=1\n",
    "            #print('hadron')\n",
    "        elif N[i]<=15 and p[i]<=100 and E[i]>=60 and HE[i]<=5:\n",
    "            num_e+=1\n",
    "            #print('electron')\n",
    "        elif p[i]>=50 and p[i]>=50 and p[i]<=120 and N[i]<=10 and E[i]<=7 and HE[i]<=25:\n",
    "            num_muon+=1\n",
    "            #print('muon')\n",
    "        elif N[i]<=15 and p[i]<=100 and E[i]>=10 and E[i]<=60 and HE[i]<=40:\n",
    "            num_tau+=1\n",
    "            #print('tau')\n",
    "    return np.array([num_had, num_muon, num_e, num_tau])\n",
    "\n",
    "#upper limit for the cos_thet cut for s- and t-channel ...\n",
    "limit=0.3\n",
    "\n",
    "#for simulated data and opal data\n",
    "def count_particles_simulated(p,N,E,HE, cos_thet): \n",
    "    num_had=0\n",
    "    num_muon=0\n",
    "    num_e=0\n",
    "    num_tau=0\n",
    "    \n",
    "    #list of cos_theta values for electrons\n",
    "    cos_thet_e=[]\n",
    "    \n",
    "    #list of s-channel electrons obtained using the cut bounderies which are determined in Ex. 2\n",
    "    e_s_channel=[]\n",
    "    \n",
    "    #list of cos_theta values for muons\n",
    "    cos_thet_muon=[]\n",
    "    \n",
    "    #using the cut conditions chosen while examining the histograms obtained from the simulation data\n",
    "    \n",
    "    for i in range(0,len(N)):\n",
    "        if N[i]>=10:\n",
    "            num_had+=1\n",
    "        elif N[i]>=7 and E[i]<=82 and E[i]>=32 and HE[i]>=1 and p[i]<=85 and p[i]>=20 and p[i]/E[i]<=5:\n",
    "            num_had+=1\n",
    "            #print('hadron')\n",
    "        #elif p[i]/E[i]>=20:\n",
    "        #    num_muon+=1\n",
    "        elif N[i]<=8 and p[i]<=75 and N[i]>=2 and E[i]<=65 and HE[i]<=22:# and p[i]/E[i]<=25:# and p[i]/HE[i]<=6:\n",
    "            num_tau+=1\n",
    "        elif p[i]>=65 and N[i]<=5 and N[i]>=1 and E[i]<=15.5 and HE[i]<=12:# and N[i]*E[i]<20 and p[i]*E[i]>50:\n",
    "            num_muon+=1\n",
    "            cos_thet_muon.append(cos_thet[i])\n",
    "        elif N[i]<=8 and p[i]<=100 and E[i]>=65 and HE[i]<=5 and p[i]>=0:# and p[i]/E[i]<=3:# and N[i]*E[i]>10:\n",
    "            num_e+=1\n",
    "            cos_thet_e.append(cos_thet[i])\n",
    "            if cos_thet[i]<=limit and cos_thet[i]>=-0.9:\n",
    "                e_s_channel.append([p[i],N[i],E[i],HE[i],cos_thet[i]])\n",
    "            #index=np.where((cos_thet_e<=0) & (cos_thet_e>=-0.9))\n",
    "            \n",
    "    return np.array([num_had, num_e, num_tau, num_muon]), cos_thet_e, cos_thet_muon,e_s_channel\n",
    "\n",
    "\n",
    "\n",
    "#Note: The function count_particles_simulated was first determined by using defined cut limits, by variation by hand\n",
    "\n",
    "#we also tried to obtain the cuts from the 3-sigma interval of gauss fits of the histograms\n",
    "#for the simulated data. However this yields, not sufficiently good results and is therefore omitted.\n",
    "\n",
    "# this is another way of counting the particles which will be compared to the upper one for the first part of the experiment\n",
    "def particletest(N,Psum,E,HE):\n",
    "    hadrons1=[]\n",
    "    electrons1=[]\n",
    "    tau1=[]\n",
    "    muon1=[]\n",
    "    \n",
    "    hadrons1.append((np.where(N>=9,2,0))) ##\n",
    "    hadrons1.append((np.where(N>=6.5,1,0)))##\n",
    "    hadrons1.append((np.where(N>=15,20,0)))##\n",
    "    hadrons1.append(np.where(Psum<=80,1,0)) ##\n",
    "    hadrons1.append((np.where((E<=65) & (E>=45),0.5,0)))##\n",
    "    hadrons1.append((np.where((E<=80) & (E>=30),1,0)))##\n",
    "    hadrons1.append(np.where(HE>=20,1,0)) ##\n",
    "    electrons1.append(np.where(HE<=20,1,0)) ##\n",
    "    electrons1.append(np.where((HE>=0.2)&(HE<=1.5),0.75,0))##\n",
    "    tau1.append(np.where(HE<=20,1,0)) ##\n",
    "    muon1.append(np.where(HE<=20,1,0)) ##\n",
    "\n",
    "    electrons1.append(np.where(N<=4.8,1,0)) ##\n",
    "    electrons1.append(np.where(N<=1.5,2,0)) ##\n",
    "    electrons1.append(np.where(Psum<=95,2.5,0)) ###\n",
    "    electrons1.append(np.where(E>=65,1.25,0)) ##\n",
    "    electrons1.append(np.where(E>=75,2,0)) ##\n",
    "    electrons1.append(np.where(E>=85,9,0)) ##\n",
    "    muon1.append(np.where(E<=15,1,0)) ##\n",
    "    muon1.append(np.where(E<=9,1.5,0))#????  ##\n",
    "    muon1.append(np.where((Psum>=70),1.5,0))\n",
    "    muon1.append(np.where( (Psum>=80),1.5,0)) ##\n",
    "    \n",
    "    muon1.append(np.where((N<=4) &(N>=1),1.5,0)) ##\n",
    "    \n",
    "    tau1.append(np.where((N<=6.5) &(N>=1.5),1,0)) ##\n",
    "    \n",
    "    tau1.append(np.where(Psum <=75,0.25,0)) ##\n",
    "    tau1.append(np.where(Psum <=70,0.5,0)) ##\n",
    "    tau1.append(np.where(Psum <=65,0.75,0)) ##\n",
    "    tau1.append(np.where(Psum <=60,1.5,0)) ##\n",
    "    tau1.append(np.where(E<=70,0.5,0)) ##\n",
    "    tau1.append(np.where((E<=25) &(E>=15),1,0)) ##\n",
    "    \n",
    "    hadrons1= np.sum(hadrons1, axis=0)\n",
    "    electrons1= np.sum(electrons1, axis=0)\n",
    "    tau1= np.sum(tau1, axis=0)\n",
    "    muon1= np.sum(muon1, axis=0)\n",
    "    \n",
    "    maximal=np.amax([[hadrons1],[electrons1],[tau1],[muon1]], axis=0)\n",
    "\n",
    "    hadron=np.sum(np.where(hadrons1>=maximal, 1,0))\n",
    "    ele=np.sum(np.where(electrons1>=maximal, 1,0))\n",
    "    tau=np.sum(np.where(tau1>=maximal, 1,0))\n",
    "    muon=np.sum(np.where(muon1>=maximal, 1,0))\n",
    "    \n",
    "    return np.array([hadron, ele, tau,muon])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Grope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists consisting of four lists including N_psum, N_char, E_ecal, E_hcal\n",
    "\n",
    "hadron=np.array([[54.8,50.0,32.0,53.3,13.9,40.4,43.3,3.2,293.1,39.8,72.0,66.2,49.7,53.8,13.2,60.2,18.9,60.9,48.7,59.8],[52,28,24,36,28,39,21,30,22,34,39,20,25,25,53,30,37,23,27,9],[39.7,55.7,46.7,33.2,58.4,58.8,50.8,37.2,52.9,50.7,41.4,23.8,37.8,53.2,51.5,37.8,58.8,53.4,57.0,55.6], [15.8,11.9,8.3,16.3,5.5,0.6,203.9,3.3,15.7,13.5,25.0,15.9,48.7,17.7,6.0,84.6,9.0,28.4,9.5,23.3]])\n",
    "electron=np.array([[0,84.4,92.2,0,78.5,83.8,0.0,0.0,71.0,0.0,71.6,71.3,74.4,0.0,83.2,47.9,65.1,82.5,0.0,0.0],[3,7,2,7,3,2,5,0,2,6,2,2,2,14,2,6,5,2,4,2],[92.0,89.1,89.5,67.9,90.3,93.5,75.3,84.1,91.6,95.3,92.2,94.2,88.5,83.8,91.2,90.3,92.5,90.3,91.1,90.1],[0.2,0.2,0.2,0.0,0.3,0.9,0.8,0.8,1.8,1.4,0.0,0.7,0.3,0.3,0.3,0.3,0.0,0.3,2.2,6.1]])\n",
    "muon=np.array([[84.7,89.5,98.2,105.1,84.1,88.1,119.1,87.3,87.7,87.0,93.0,80.9,92.3,89.0,93.4,94.3,86.5,90.3,83.2,96.1],[6,2,2,2,2,2,2,4,2,2,2,2,4,2,2,7,2,2,2,2],[1.4,1.6,2.6,1.8,5.4,1.5,2.1,5.3,3.0,1.8,1.3,3.1,1.7,3.1,1.7,1.3,6.2,2.5,1.8,1.8],[2.4,13.4,9.9,6.6,13.8,3.5,6.2,30.3,9.3,10.0,2.6,4.7,5.5,5.3,1.8,3.9,44.7,18.8,4.9,10.7]])\n",
    "tau=np.array([[43.8,31.4,43.0,37.6,55.0,33.9,0,44.7,2.0,67.0,99.3,45.8,0.0,30.4,41.1,26.9,64.6,44.9,25.0,29.8],[4,2,2,5,2,2,3,14,6,6,2,4,8,2,2,2,4,13,2,2],[8.2,36.1,31.7,43.0,49.7,52.0,31.1,35.7,90.8,45.0,16.6,40.1,70.8,21.7,40.6,51.8,29.8,37.5,44.9,36.5],[2.0,0.2,2.4,0.6,36.8,13.6,0.3,3.3,0.7,33.6,16.8,38.2,0,7.5,2.6,0.0,9.2,2.2,4.1,23.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(mplhep.style.ATLAS)\n",
    "fig, ax = plt.subplots(figsize = (15, 8))\n",
    "\n",
    "title=['Sum of Momenta of Charged Tracks', 'Number of Charged Tracks', 'Energies em. Calorimeter', 'Energies Hadron Calorimeter']\n",
    "label=['Hadrons', 'Electrons', 'Taus', 'Muons']\n",
    "bins=[[15,8,3,10],[15,6,6,6],[10,10,2,15],[18,5,5,5]]\n",
    "ranges=[(0,300),(0,60), (0,100), (0,100)]\n",
    "\n",
    "\n",
    "fig.text(0.1025, 0.5, r'Particles per Bin', va='center', rotation='vertical', fontsize=15)\n",
    "for i in [0,1,2,3]: # 0=charged track sump, 1=charged track N, E\n",
    "    plt.subplot(221+i)\n",
    "    plt.title(title[i], fontsize=15)\n",
    "    for j in [0,1,2,3]:\n",
    "        particle=[hadron[i], electron[i],muon[i],tau[i]]\n",
    "        plt.hist(particle[j], bins=30, label=label[j],range=ranges[i], edgecolor='black',alpha=1)\n",
    "    plt.grid()\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###testing the cuts for Grope data\n",
    "\n",
    "p=np.concatenate((hadron[0], electron[0], muon[0], tau[0]))\n",
    "N=np.concatenate((hadron[1], electron[1], muon[1], tau[1]))\n",
    "E=np.concatenate((hadron[2], electron[2], muon[2], tau[2]))\n",
    "HE=np.concatenate((hadron[3], electron[3], muon[3], tau[3]))\n",
    "\n",
    "#method 1\n",
    "cp=count_particles(p,N,E,HE)\n",
    "#this method throws some particles away and counts most of the time less particles than there actually are\n",
    "\n",
    "print('numbers of hadrons, electron, taus, muons:',cp)\n",
    "\n",
    "#method two counts in general more particles than there actually are\n",
    "#method two is not displayed here but you can see the final result obtained for the cross sections below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating data obtained by Monte Carlo simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the frist part of the expoeriment we will investigate some simulations of Data sets with particles of known origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will open data and Monte Carlo samples using **uproot**. Uproot is a reader and a writer of the ROOT file format using only Python and Numpy. Unlike PyROOT and root_numpy, uproot does not depend on C++ ROOT so that no local compilation of the ROOT libraries is needed to access the data.\n",
    "\n",
    "You can find more info on uproot following the references:\n",
    "* Github repo: https://github.com/scikit-hep/uproot4\n",
    "* Tutorial: https://masonproffitt.github.io/uproot-tutorial/\n",
    "* Video tutorial on uproot and awkward arrays:  https://www.youtube.com/embed/ea-zYLQBS4U \n",
    "\n",
    "First, let's specify the folder path for both data and Monte Carlo (MC) samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "In this section we want to get a rough estimate of how our counting algorithm works. Therefore we use the function we generated above for our cutting, to get a rough estimate of how many particles are actually counted wrong.\n",
    "\n",
    "In the below discussion we did not include any errors, which we will get through the Poisson distribution.\n",
    "\n",
    "Based on the new Histogramms, whcih are shown in the end of the chapter, we will define new cutting regions for our particles. We do poisson fits and then include alle data in a 3-sigma intervall. \n",
    "\n",
    "It turned out that the results provided by this method for the intervals, where to det the cuts is more bad, than just estimating by hand and screwing a bit with the parameters of the fit itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last line shows all the variables available in the TTree to carry out the experiment. The meaning of these is described in the following table\n",
    "\n",
    "| Variable name | Description |\n",
    "| --- | --- | \n",
    "| <pre>run</pre> | Run number |\n",
    "| <pre>event</pre> | Event number |\n",
    "| <pre>Ncharged</pre> | Number of charged tracks |\n",
    "| <pre>Pcharged</pre> | Total scalar sum of track momenta |\n",
    "| <pre>E_ecal</pre> | Total energy measured in the electromagnetic calorimeter |\n",
    "| <pre>E_hcal</pre> | Total energy measured in the hadronic calorimete |\n",
    "| <pre>E_lep</pre> | LEP beam energy (=$\\sqrt{s}/2$) |\n",
    "| <pre>cos_thru</pre> | cosine of the polar angle between beam axis and thrust axis |\n",
    "| <pre>cos_thet</pre> | cosine of the polar angle between incoming positron and outgoing positive particle |\n",
    "\n",
    "We proceed to plot *PCharged* for illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading simulated data and creating efficiency matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix obtained using counting function 1\n",
    "matrix=[]\n",
    "error_matrix=[]\n",
    "# matrix obtained using counting function 2\n",
    "matrix2=[]\n",
    "error_matrix2=[]\n",
    "\n",
    "#list of criterias for each particle type (Nchar, ....)\n",
    "criteria=[]\n",
    "\n",
    "\n",
    "for i in ['qq.root','ee.root', 'tt.root','mm.root']:\n",
    "    ### Open the file introducing file path\n",
    "    file = uproot.open(path_data+i)\n",
    "    ttree_name = 'myTTree'\n",
    "\n",
    "    ### Print list of 'branches' of the TTree (i.e. list of variable names)\n",
    "    file[ttree_name].keys()\n",
    "    #print(file[ttree_name].keys())\n",
    "\n",
    "    ## Load branches\n",
    "    branches = file[ttree_name].arrays()\n",
    "\n",
    "    ## Define an numpy array for 'Pcharged'\n",
    "    Pcharged = 'Pcharged'\n",
    "    NCharged='Ncharged'\n",
    "    E_ecal='E_ecal'\n",
    "    E_hcal='E_hcal'\n",
    "    cos_thet='cos_thet'\n",
    "    E_lep='E_lep'\n",
    "    \n",
    "\n",
    "    pchar = ak.to_numpy(branches[Pcharged]) # See Docu (https://awkward-array.org/how-to-convert-numpy.html) for more conversions\n",
    "    Nchar = ak.to_numpy(branches[NCharged])\n",
    "    E_ecal = ak.to_numpy(branches[E_ecal])\n",
    "    E_hcal = ak.to_numpy(branches[E_hcal])\n",
    "    cos_thet =ak.to_numpy(branches[cos_thet])\n",
    "    E_lep = ak.to_numpy(branches[E_lep])\n",
    "    \n",
    "    criteria.append([pchar, Nchar, E_ecal, E_hcal, cos_thet,E_lep])\n",
    "    \n",
    "    cp,ph1,ph2,ph3=count_particles_simulated(pchar,Nchar,E_ecal,E_hcal,cos_thet) #ph= placeholder\n",
    "    \n",
    "    #inserting counted particle number divided through total particle number into the matrix\n",
    "    matrix.append(cp/len(Nchar))\n",
    "    \n",
    "    \n",
    "    #the errors are calculated assuming a poisson distribution of the counted numbers\n",
    "    #which is justified since all considered \n",
    "    # numbers are larger than 30 which is considered as ''very large'' \n",
    "    error_matrix.append(np.sqrt(cp/len(Nchar)**2+cp**2/len(Nchar)**3))\n",
    "    \n",
    "    listen=np.array(np.full(4,0))\n",
    "    cp=particletest(Nchar,pchar,E_ecal,E_hcal)\n",
    "    matrix2.append(cp/len(Nchar))\n",
    "    error_matrix2.append(np.sqrt(cp/len(Nchar)**2+cp**2/len(Nchar)**3))\n",
    "    \n",
    "\n",
    "matrix=np.array(matrix)\n",
    "matrix2=np.array(matrix2)\n",
    "error_matrix=np.array(error_matrix)   \n",
    "error_matrix2=np.array(error_matrix2)\n",
    "\n",
    "\n",
    "print('Efficiency matrix:')\n",
    "print(matrix)\n",
    "print('Error efficiency matrix:')\n",
    "print(error_matrix)\n",
    "print('Efficiency matrix 2:')\n",
    "print(matrix2)\n",
    "print('Error efficiency matrix 2:')\n",
    "print(error_matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all histograms in one\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 8))\n",
    "title=['Sum of Momenta of Charged Tracks', 'Number of Charged Tracks', 'Energies em. Calorimeter', 'Energies Hadron Calorimeter']\n",
    "label=['Hadrons', 'Electrons', 'Taus', 'Muons']\n",
    "xlim=[(0,120),(0,50), (0,110), (0,40)]\n",
    "ylim=[(0,0.8e4),(0,0.5e5),(0,1.7e4),(0,1.3e4)]\n",
    "#fig.text(0.55, 0.1, r'Center of Mass Energy [GeV]', ha='center', fontsize=15)\n",
    "fig.text(0.1025, 0.5, r'Particles per Bin', va='center', rotation='vertical', fontsize=15)\n",
    "bins=[200,140,200,200]\n",
    "\n",
    "for i in [0,1,2,3]:\n",
    "    plt.subplot(221+i)\n",
    "    plt.title(title[i], fontsize=15)\n",
    "    for j in [0,1,2,3]:\n",
    "        particle=[criteria[0][i], criteria[1][i],criteria[2][i],criteria[3][i]]\n",
    "        entries, bin_edges, patches=plt.hist(particle[j], bins=bins[i],range=(0.,140.), label=label[j], edgecolor='black',alpha=0.3)\n",
    "        plt.xlim(xlim[i])\n",
    "        plt.ylim(ylim[i])\n",
    "    plt.legend()\n",
    "    plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplary gauss fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one histogram with gauss fits\n",
    "\n",
    "poptp0Ecal=([(0.3e4,55,20),(0.7e4,90,10),(0.1e4,35,40),(0,30,30)])\n",
    "color=['lightgreen', 'lightblue', 'yellow', 'pink']\n",
    "color2=['green', 'blue', 'orange', 'darkred']\n",
    "testho=[0.5e4,0.9e4,0.4e4,2e4]\n",
    "#KäsefürFehler=0.1\n",
    "for i in [2]:\n",
    "    plt.figure()\n",
    "    plt.title(title[i])\n",
    "    for j in [0,1,2,3]:\n",
    "        particle=[criteria[0][i], criteria[1][i],criteria[2][i],criteria[3][i]]\n",
    "        entries, bin_edges, patches=plt.hist(particle[j], bins=200,range=(0,100), label=label[j],color=color[j], edgecolor='black',alpha=0.5)\n",
    "        x=np.linspace(min(particle[j]),max(particle[j]),600)\n",
    "        bin_middles = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "        popt, pcov = curve_fit(gauss, bin_middles, entries,p0=poptp0Ecal[j],maxfev=10000)\n",
    "        #popt2, pcov2=curve_fit(fit_function, bin_middles, entries, maxfev=1000000)         \n",
    "        #plt.plot(x,fit_function(x,*popt2))\n",
    "        plt.plot(x,gauss(x,*popt), color=color2[j])\n",
    "       # print('___________________________________________________________________________________________________________________')\n",
    "       # print('Die fitparameter der Gaußfunktion des Histograms', label[j],'mit zugehörigen Fehlern lauten:','\\n'.join((\n",
    "       # r'$A=(%.3f \\pm %.3f)$' % (popt[0],np.sqrt(pcov[0][0]), ),\n",
    "       # r'$\\mu=(%.3f \\pm %.3f)$' % (popt[1],np.sqrt(pcov[1][1]), ),\n",
    "       # r'$\\sigma=(%.3f \\pm %.3f)$' % (popt[2],np.sqrt(pcov[2][2]), ),)))\n",
    "       # print('Als Cutting bereich für', title[i],'für', label[j],'wählen wir die Bereiche:',popt[1]-3*abs(popt[2]),popt[1]+3*abs(popt[2]))\n",
    "       # print('___________________________________________________________________________________________________________________')\n",
    "        #redchi=redchi2(entries,gauss(bin_middles,*popt),np.sqrt(entries)+KäsefürFehler,3)\n",
    "\n",
    "\n",
    "        textstr = '\\n'.join((\n",
    "            r'$A=(%.0f \\pm %.0f)$' % (popt[0],np.sqrt(pcov[0][0]), ),\n",
    "            r'$\\mu=(%.2f \\pm %.2f)$' % (popt[1],np.sqrt(pcov[1][1]), ),\n",
    "            r'$\\sigma=(%.2f \\pm %.2f)$' % (popt[2],np.sqrt(pcov[2][2]), )))\n",
    "           # r'$\\chi^2_{red}=%.1f$' % (redchi, )))\n",
    "\n",
    "        plt.text(popt[1]+10, testho[j], textstr, size=10,ha=\"center\",\n",
    "            va=\"center\",bbox=dict(boxstyle=\"round\",facecolor=color[j], edgecolor=color2[j], ), color='black')\n",
    "        \n",
    "        plt.xlim(0,115)\n",
    "        plt.ylim(0,2.8e4)\n",
    "    plt.legend()\n",
    "    plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single histograms for appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2,3]:\n",
    "    plt.figure()\n",
    "    plt.title(title[i])\n",
    "    for j in [0,1,2,3]:\n",
    "        particle=[criteria[0][i], criteria[1][i],criteria[2][i],criteria[3][i]]\n",
    "        entries, bin_edges, patches=plt.hist(particle[j],bins=bins[i],range=(0.,140.), edgecolor='black', label=label[j],alpha=0.5)\n",
    "        plt.ylim(ylim[i])\n",
    "        plt.xlim(xlim[i])\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inversion\n",
    "To determine the uncertainties of the matrix elements after the inversion we use Monte Carlo toy experiments. In this context, what are the advantages and disadvantages of this method when compared to analytical expressions? Discuss it briefly.\n",
    "\n",
    "**References**:\n",
    "* Propagation of Errors for Matrix Inversion: https://arxiv.org/abs/hep-ex/9909031v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Number of toy experiments to be done\n",
    "ntoy = 1000\n",
    "\n",
    "### Create numpy matrix of list to append elements of inverted toy matrices\n",
    "inverse_toys = np.empty((4,4))\n",
    "\n",
    "# Create toy efficiency matrix out of gaussian-distributed random values\n",
    "for i in range(0,ntoy,1):\n",
    "    toy_matrix = np.zeros((4,4))\n",
    "    #np.random.seed(2)\n",
    "    toy_matrix = np.random.normal(matrix,error_matrix,size=(4,4))\n",
    "    \n",
    "    ### Invert toy matrix\n",
    "    inverse_toy = np.linalg.inv(toy_matrix)\n",
    "    \n",
    "    #print(inverse_toys.item(0,0),inverse_toy.item(0,0))\n",
    "    # Append values\n",
    "    \n",
    "    inverse_toys = np.dstack((inverse_toys,inverse_toy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gaussian function to fit to the toy distributions:\n",
    "\n",
    "\n",
    "\n",
    "inverse_errors = np.zeros((4,4))\n",
    "inverse_means = np.zeros((4,4))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12),dpi=80)\n",
    "fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.5)\n",
    "#fig.tight_layout()\n",
    "ax00 = plt.subplot(4,4,1)\n",
    "ax01 = plt.subplot(4,4,2)\n",
    "ax02 = plt.subplot(4,4,3)\n",
    "ax03 = plt.subplot(4,4,4)\n",
    "\n",
    "ax10 = plt.subplot(4,4,5)\n",
    "ax11 = plt.subplot(4,4,6)\n",
    "ax12 = plt.subplot(4,4,7)\n",
    "ax13 = plt.subplot(4,4,8)\n",
    "\n",
    "ax20 = plt.subplot(4,4,9)\n",
    "ax21 = plt.subplot(4,4,10)\n",
    "ax22 = plt.subplot(4,4,11)\n",
    "ax23 = plt.subplot(4,4,12)\n",
    "\n",
    "ax30 = plt.subplot(4,4,13)\n",
    "ax31 = plt.subplot(4,4,14)\n",
    "ax32 = plt.subplot(4,4,15)\n",
    "ax33 = plt.subplot(4,4,16)\n",
    "\n",
    "axes = [[ax00,ax01,ax02,ax03],\n",
    "        [ax10,ax11,ax12,ax13],\n",
    "        [ax20,ax21,ax22,ax23],\n",
    "        [ax30,ax31,ax32,ax33]]\n",
    "\n",
    "## IMPORTANT! Find suitable ranges to fit/plot gaussian distributions successfully!\n",
    "ranges = [[(1.005,1.03)   ,(-0.0009,-0.0002), (-0.008,-0.0055), (0.0000018,0.000006)],\n",
    "          [(0.000032,0.000052),(1.05,1.075)   , (-0.0065,-0.0045), (0.0000017,0.0000046)],\n",
    "          [(-0.01,-0.007),(-0.024,-0.019), (1.1,1.13)   , (-0.001,-0.00027)],\n",
    "          [(0.00071,0.00092),(0.00187,0.00225), (-0.112,-0.103), (1.13,1.15)]]\n",
    "\n",
    "\n",
    "p0=[[[15,1.02,0.05],[20,-0.00055,0.00001],None, [26,0.000004,0.000001]],\n",
    "    [[20,0.000044,0.000002],[20,1.062,0.004],None,[25,0.0000035,0.0000005]],\n",
    "   [[110,-0.0085,0.0005],None,None,[28,-0.0007,0.00005]],\n",
    "  [[90,8.2e-4,0.5e-4], [90,2.1e-3,0.5e-3], None, None]]\n",
    "\n",
    "# Fill histograms for each inverted matrix coefficient:\n",
    "for j in range(0,4,1):\n",
    "    for k in range(0,4,1):\n",
    "        \n",
    "        # Diagonal and off-diagonal terms have different histogram ranges\n",
    "        hbins, hedges, _ = axes[j][k].hist(inverse_toys[j,k,:],bins=30,range=ranges[j][k], edgecolor='black', linewidth=2, label=f'toyhist{j}{k}')\n",
    "        axes[j][k].legend()\n",
    "\n",
    "        ## Guess initial parameters of the fit by taking random value from hist and std\n",
    "        _p0 = [ntoy/10.,np.mean(inverse_toys[j,k,:]),np.std(inverse_toys[j,k,:])]\n",
    "\n",
    "        # Get the fitted curve\n",
    "        h_mid = 0.5*(hedges[1:] + hedges[:-1]) #Calculate midpoints for the fit\n",
    "        coeffs, _ = curve_fit(gauss, h_mid, hbins, maxfev=10000,p0=p0[j][k])\n",
    "        h_fit = gauss(h_mid, *coeffs)\n",
    "        \n",
    "        axes[j][k].plot(h_mid, h_fit,label=f'Fit{j}{k}')\n",
    "\n",
    "        inverse_means[j,k] = coeffs[1]\n",
    "        inverse_errors[j,k] = abs(coeffs[2])\n",
    "        axes[j][k].set_xlim(ranges[j][k])\n",
    "\n",
    "print(f\"Erros for the inverse matrix:\\n{inverse_errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate inverse matrices for the first counting methods (count_particles_simulated)\n",
    "\n",
    "inverse_matrix1=np.linalg.inv(matrix)\n",
    "inverse_matrix2=np.linalg.inv(matrix2)\n",
    "\n",
    "print(f\"The inverse matrix:\\n{np.linalg.inv(matrix)}\")\n",
    "print(f\"Erros for the inverse matrix:\\n{inverse_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in OPAL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining new criteria list\n",
    "\n",
    "criteria2=[]\n",
    "\n",
    "\n",
    "\n",
    "#actual energies found in data\n",
    "#we obtained these energies by outputting E_lep and picking out all different energies\n",
    "#then we rounded these eneregies up to one number after the comma\n",
    "#these gave us the seven energy values we are interested in\n",
    "\n",
    "energy=[88.4,89.4,90.2,91.2,92,93,93.8]\n",
    "\n",
    "##########################################\n",
    "\n",
    "for i in range(0, len(energy)):\n",
    "    ### Open the file introducing file path\n",
    "    file = uproot.open(path_data+'daten_2.root')\n",
    "    ttree_name = 'myTTree'\n",
    "\n",
    "    ### Print list of 'branches' of the TTree (i.e. list of variable names)\n",
    "    file[ttree_name].keys()\n",
    "    #print(file[ttree_name].keys())\n",
    "\n",
    "    ## Load branches\n",
    "    branches = file[ttree_name].arrays()\n",
    "\n",
    "    ## Define an numpy array for 'Pcharged', etc.\n",
    "    Pcharged = 'Pcharged'\n",
    "    NCharged='Ncharged'\n",
    "    E_ecal='E_ecal'\n",
    "    E_hcal='E_hcal'\n",
    "    cos_thet='cos_thet'\n",
    "    E_lep ='E_lep'\n",
    "    pchar = ak.to_numpy(branches[Pcharged]) # See Docu (https://awkward-array.org/how-to-convert-numpy.html) for more conversions\n",
    "    Nchar = ak.to_numpy(branches[NCharged])\n",
    "    E_ecal = ak.to_numpy(branches[E_ecal])\n",
    "    E_hcal = ak.to_numpy(branches[E_hcal])\n",
    "    cos_thet =ak.to_numpy(branches[cos_thet])\n",
    "    E_lep = ak.to_numpy(branches[E_lep])\n",
    "    \n",
    "    ################################################\n",
    "    #sorting the single criteria by the energies from the list above\n",
    "    #by checking wether their E_lep entry equals the energy of the list which is considered in this for loop\n",
    "\n",
    "    pchar=np.where(np.around(E_lep,1)==energy[i]/2, pchar, 'hey')\n",
    "    \n",
    "    #### all pchar entries whoms corresponding energy does not fit are replaced by 'hey', and removed from the list in the\n",
    "    #next step\n",
    "    pchar= np.asarray(pchar[pchar != 'hey'], dtype=np.float64)\n",
    "    \n",
    "    #this, we repeat for all criterias\n",
    "    \n",
    "    Nchar=np.where(np.around(E_lep,1)==energy[i]/2, Nchar, 'hey')\n",
    "    Nchar= np.asarray(Nchar[Nchar != 'hey'], dtype=np.float64)\n",
    "    E_ecal=np.where(np.around(E_lep,1)==energy[i]/2, E_ecal, 'hey')\n",
    "    E_ecal= np.asarray(E_ecal[E_ecal != 'hey'], dtype=np.float64)\n",
    "    E_hcal=np.where(np.around(E_lep,1)==energy[i]/2, E_hcal, 'hey')\n",
    "    E_hcal=np.asarray(E_hcal[E_hcal != 'hey'], dtype=np.float64)\n",
    "    cos_thet=np.where(np.around(E_lep,1)==energy[i]/2, cos_thet, 'hey')\n",
    "    cos_thet= np.asarray(cos_thet[cos_thet != 'hey'], dtype=np.float64)\n",
    "    \n",
    "    #again we include into criteria all the criteria for all the particles for all the energies\n",
    "    # thus critera[num] gives all entries for one energy,\n",
    "    #for one energy and criteria[num][num] the actual criteria for all particles for the considered energy\n",
    "    criteria2.append([pchar, Nchar, E_ecal, E_hcal, cos_thet])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Separate $t$- and $s$-channel contributions\n",
    "\n",
    "Only Feynman diagrams contributing to the production of $Z$ boson are to be considered for the measurements. The **electron** Monte Carlo sample incorporate contributions from $t$- and $s$-channels.\n",
    "* Select/correct contributions producing $Z$ boson decays. (Hint: Which role does the $\\cos(\\theta)$ distribution play in separating $t$- and $s$-channels?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperation for simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to seperate s- and t-channel we fit the below function onto a part of the histogram\n",
    "#and use each summend of the function to sepearte s- and t-channel since one summand described s- and the other t-channel\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(r'Selection of the $t$- and $s$- Channel Contributions', fontsize=17)\n",
    "plt.xlabel(r'angle of detection $\\cos{\\theta}$', fontsize=15)\n",
    "    \n",
    "entries, bin_edges, patches=plt.hist(criteria[1][4], bins=500,range=(-1,1),alpha=0.5)\n",
    "x=np.linspace(-0.99,0.99,len(entries))\n",
    "    \n",
    "bin_middles = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "popt, pcov= curve_fit(f,bin_middles[17:-20],entries[17:-20],sigma=np.sqrt(entries[17:-20]),p0=[60.7,7.2], maxfev=2000)\n",
    "plt.plot(x, f(x, *popt), label='total')\n",
    "plt.plot(x, popt[0]*(1+x**2), label='s-channel')\n",
    "plt.plot(x, popt[1]/(1-x)**2, label='t-channel')\n",
    "         \n",
    "         \n",
    "redchi=redchi2(entries[17:-20],f(bin_middles[17:-20],*popt),np.sqrt(entries[17:-20]),2)\n",
    "\n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    r'$s=(%.1f \\pm %.1f) $' % (popt[0],np.sqrt(pcov[0][0]), ),\n",
    "    r'$t=(%.1f \\pm %.1f) $ ' % (popt[1],np.sqrt(pcov[1][1]), ),\n",
    "    r'$\\chi^2_{red}=%.1f$' % (redchi, )))\n",
    "\n",
    "plt.text(0, 600, textstr, size=15,ha=\"center\",\n",
    "         va=\"center\",bbox=dict(boxstyle=\"round\",facecolor='lightblue', edgecolor='darkblue', ), color='black')\n",
    "\n",
    "plt.xlim(-1.1,1.1)\n",
    "plt.ylim(0,1000)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "#Calculating the factor to pick out s-channel contributions\n",
    "#########################################################################\n",
    "#first try:\n",
    "\n",
    "u=1\n",
    "l=-1\n",
    "\n",
    "#we calculate the efficiency by dividing the integral of the s-channel function from -1 to 1\n",
    "#by the overall number of counted particles divided by bin_number/interval length(=500/2)\n",
    "\n",
    "\n",
    "#we calll the efficiency ''cleaned'' because we consider the simualted data where we know for sure that we are considering \n",
    "#electrons\n",
    "\n",
    "efficiency_clean=popt[0]*((u+1/3*u**3)-(l+1/3*l**3))/(sum(entries)/250)\n",
    "\n",
    "#this efficiency is however not used since it does not yield good results\n",
    "#the cross section vs mean energy plot for electrons becomse different (the first two cross sections are to high)\n",
    "######################################################################\n",
    "#second try: cutting method\n",
    "\n",
    "#in order to fix the problem mentioned above we chose the simply cut a part out of the plot which yields a high efficiency:\n",
    "\n",
    "#interval limits:\n",
    "u=limit\n",
    "l=-0.9\n",
    "# thus we throw away all electrons which have a costheta larger than u and smaller than l\n",
    "\n",
    "\n",
    "efficiency_clean2=popt[0]*((u+1/3*u**3)-(l+1/3*l**3))/(sum(entries[25:300])/250)\n",
    "\n",
    "c1=((u+1/3*u**3)-(l+1/3*l**3))\n",
    "c2=(1/(1-u)-1/(1-l))\n",
    "\n",
    "# instead of dividing by the total number of counted particles we integrate the fit function f(..)\n",
    "# from l to u and divide by the hereby obtained number to get the efficiency\n",
    "\n",
    "efficiency_clean3=(popt[0]*c1)/(popt[0]*c1+popt[1]*c2)\n",
    "defficiency_clean3=np.sqrt((efficiency_clean3**2*c2)**2*pcov[1][1]+((efficiency_clean3**2*c1/(popt[0]))**2*pcov[0][0]))\n",
    "#this last version is the one we will use in the following\n",
    "\n",
    "######################################################################################\n",
    "#recovering what we lose due to wrong assigment while detecting (data with costheta=999)\n",
    "#beta=len(cos_thet_ee)/sum(entries)\n",
    "#dbeta=np.sqrt(len(cos_thet_ee)/sum(entries)**2+(len(cos_thet_ee)/sum(entries)**2)**2*sum(entries))\n",
    "# Note cos_thet_ee is defined in the following\n",
    "#we omit this idea since multiplying the electron number by this\n",
    "# worsens the result\n",
    "#which is probably due to the fact that most of the wrongly assigned electrons are t-channel electrons\n",
    "\n",
    "############################################################################################\n",
    "#in order to fix the electron number by the number of ''interference'' electrons we have just cut out/removed in the above\n",
    "#step, we calculate the ratio of the integral of the s-channel function over the whole interval and the integral over the\n",
    "#limits we have chosen above\n",
    "\n",
    "uu=1\n",
    "ll=-1\n",
    "a=popt[0]*((u+1/3*u**3)-(l+1/3*l**3))\n",
    "b=popt[0]*((uu+1/3*uu**3)-(ll+1/3*ll**3))\n",
    "\n",
    "ratio=b/a\n",
    "#this ratio does not have an error since it only depends on the chosen limits\n",
    "\n",
    "print('efficiency: (%.3f +/- %.3f)'%(efficiency_clean3,defficiency_clean3) )\n",
    "print('fixing ratio: %.3f' %(ratio))\n",
    "\n",
    "\n",
    "###Notes:\n",
    "#beta was removed since it lead to wrong values, so the electrons which\n",
    "#were not correctly registered by the detetor should indeed not be considered\n",
    "#since they are probably mostly t-channel contributions, i.e. have very smaller theta values, such that the detector\n",
    "#can not measure them\n",
    "#(thats why the electron cross section is still as large as muon and taon cross section although we threw away half of the\n",
    "#electrons\n",
    "####################################\n",
    "#note that changeing the integral limits changes the Z-mass for electrons, the smaller, the better\n",
    "#it also changes the line width of all fermions, when trying to optimize the electron line width by making the\n",
    "#intervall smaller, tau and muon line width worsen, thus one has to find a compromis\n",
    "\n",
    "#moreover the number of leptons depends strongly on these intervals, such that enlarging the interval by a small amount\n",
    "#yields two instead of three neutrinos\n",
    "\n",
    "##################################\n",
    "#we also tried to multiply the efficiency to the initial matrix, i.e. before inverting but this did not really yield \n",
    "#better results, it worsened the results for tau and muon but made them be less dependend of the cut interval for the\n",
    "#t-s-channel seperation since here the actual electron entries were corrected in the matrix (before inverting), such that \n",
    "#the muon and tuon entries do not get effected this much....\n",
    "\n",
    "######\n",
    "#systematic errors: by cutting of an interval for s-t-channel, interference terms are neglected, this might lead to some\n",
    "#deviation\n",
    "#moreover half of the electrons are not measured correctly, this just leads to smaller number of overall electrons but\n",
    "#correcting this by multiplying with beta worsens the cross-section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#angular distribution for all particles \n",
    "#the hadron's distribution looks like this since most of the hadron's costheta are set to 999\n",
    "#the number of misassignments in the hadronic channel is very large\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "plt.suptitle(r'$cos \\theta $ values for all particles', fontsize=20, y=1.05)\n",
    "fig.text(0.5, 0.08,r'angle of detection $\\cos{\\theta}$', va='center', rotation='horizontal', fontsize=20)\n",
    "fig.text(0.1, 0.5, r'number of particles per bin', va='center', rotation='vertical', fontsize=20)\n",
    "label=['Hadrons', 'Electrons', 'Taus', 'Muons']\n",
    "for i in [0,1,2,3]:\n",
    "    plt.subplot(221+i)\n",
    "    entries, bin_edges, patches=plt.hist(criteria[i][4], bins=20,range=(-1,1),edgecolor='black',alpha=0.5, label=label[i])\n",
    "    x=np.linspace(-0.99,0.99,len(entries))\n",
    "\n",
    "    bin_middles = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "    plt.xlim(-1.1,1.1)\n",
    "    #plt.ylim(0,1000)\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperation for OPAL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of the cos_theta values of electrons and a list including only s-channel electrons\n",
    "\n",
    "cos_thet_e=[]\n",
    "e_s_channel=[]\n",
    "for i in range(0, 7):\n",
    "    ph1, cos_thet_e_i, ph2, e_s_channel_i=(count_particles_simulated(criteria2[i][0],criteria2[i][1],criteria2[i][2],criteria2[i][3],criteria2[i][4]))\n",
    "\n",
    "    \n",
    "    cos_thet_e.append(cos_thet_e_i)\n",
    "    e_s_channel.append(e_s_channel_i)\n",
    "\n",
    "\n",
    "#since the cos_thet_e obtained above are seperated by their energies they have to be merged:\n",
    "cos_thet_ee=np.concatenate((cos_thet_e[0],cos_thet_e[1],cos_thet_e[2],cos_thet_e[3],cos_thet_e[4],cos_thet_e[5],cos_thet_e[6]))\n",
    "\n",
    "\n",
    "ee_s_channel=np.concatenate((e_s_channel[0],e_s_channel[1],e_s_channel[2],e_s_channel[3],e_s_channel[4],e_s_channel[5],e_s_channel[6]))\n",
    "\n",
    "#picking out the cos_thet values from the s-channel electron data:\n",
    "\n",
    "cos_thet_s_channel=[]\n",
    "\n",
    "for i in range(0,len(ee_s_channel)):\n",
    "    cos_thet_s_channel.append(ee_s_channel[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(r'Measured $cos \\theta $ for electrons')\n",
    "\n",
    "bins=200\n",
    "\n",
    "#creating a histogram for all electrons and the electrons picked out for being most probably s-channel electrons (red)\n",
    "entries, bin_edges, patches=plt.hist(cos_thet_ee, bins=bins,range=(-1,1),alpha=0.5)\n",
    "entries2, bin_edges2, patches=plt.hist(cos_thet_s_channel, bins=200,range=(-1,1),alpha=0.5, color='red')\n",
    "\n",
    "\n",
    "x=np.linspace(-0.99,0.99,len(entries))\n",
    "    \n",
    "bin_middles = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "bin_middles2 = 0.5 * (bin_edges2[1:] + bin_edges2[:-1])\n",
    "\n",
    "#Fit\n",
    "popt, pcov= curve_fit(f,bin_middles[17:-20],entries[17:-20],sigma=np.sqrt(entries[17:-20]),p0=[60.7,7.2], maxfev=2000)\n",
    "\n",
    "\n",
    "plt.plot(x, f(x, *popt), label='total')\n",
    "plt.plot(x, popt[0]*(1+x**2), label='s-channel')\n",
    "plt.plot(x, popt[1]/(1-x)**2, label='t-channel')         \n",
    "         \n",
    "\n",
    "\n",
    "redchi=redchi2(entries[17:-20],f(bin_middles[17:-20],*popt),np.sqrt(entries[17:-20]))\n",
    "\n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    r'$s=(%.1f \\pm %.1f) $' % (popt[0],np.sqrt(pcov[0][0]), ),\n",
    "    r'$t=(%.1f \\pm %.1f) $ ' % (popt[1],np.sqrt(pcov[1][1]), ),\n",
    "    r'$\\chi^2_{red}=%.0f$' % (redchi, )))\n",
    "\n",
    "plt.text(0, 200, textstr, size=15,ha=\"center\",\n",
    "         va=\"center\",bbox=dict(boxstyle=\"round\",facecolor='lightblue', edgecolor='darkblue', ), color='black')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlim(-1.1,1.1)\n",
    "plt.ylim(0,420)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "u=limit\n",
    "\n",
    "l=-0.9\n",
    "\n",
    "#efficiency obtained using method 3 from above\n",
    "#\"dirty\" because the data includes wrongly assigned electrons\n",
    "\n",
    "c1=((u+1/3*u**3)-(l+1/3*l**3))\n",
    "c2=(1/(1-u)-1/(1-l))\n",
    "\n",
    "efficiency_dirty3=popt[0]*((u+1/3*u**3)-(l+1/3*l**3))/((popt[0]*((u+1/3*u**3)-(l+1/3*l**3))+popt[1]/(1-u)-popt[1]/(1-l)))\n",
    "defficiency_dirty3=np.sqrt((efficiency_dirty3**2*c2)**2*pcov[1][1]+((efficiency_dirty3**2*c1/(popt[0]))**2*pcov[0][0]))\n",
    "\n",
    "#recovering what we lose due to wrong assigment\n",
    "#beta=len(cos_thet_ee)/sum(entries)     #omitted\n",
    "#dbeta=np.sqrt(len(cos_thet_ee)/sum(entries)**2+(len(cos_thet_ee)/sum(entries)**2)**2*sum(entries))\n",
    "\n",
    "\n",
    "#Calculating the ratio like above\n",
    "uu=1\n",
    "ll=-1\n",
    "a=popt[0]*((u+1/3*u**3)-(l+1/3*l**3))\n",
    "b=popt[0]*((uu+1/3*uu**3)-(ll+1/3*ll**3))\n",
    "ratio_dirty=b/a\n",
    "\n",
    "print('efficiency: (%.3f +/- %.3f)'%(efficiency_dirty3,defficiency_dirty3) )\n",
    "print('fixing ratio: %.3f' %(ratio_dirty))\n",
    "\n",
    "\n",
    "#since the data contains misassigned electrons we will use the efficiency and ratio calculated above (from the simulation)\n",
    "#although the obtained results are almost the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta was removed since it lead to wrong values, so the electrons which\n",
    "#were not correctly registered by the detetor should indeed not be considered\n",
    "#since they are probably mostly t-channel contributions, i.e. have very smaller theta values, such that the detector\n",
    "#can not measure them\n",
    "#(thats why the electron cross section is still as large as muon and taon cross section although we threw away half of the\n",
    "#electrons\n",
    "####################################\n",
    "#note that changeing the integral limits changes the Z-mass for electrons, the smaller, the better\n",
    "#it also changes the line width of all fermions, when trying to optimize the electron line width by making the\n",
    "#intervall smaller, tau and muon line width worsen, thus one has to find a compromis\n",
    "\n",
    "#moreover the number of leptons depends strongly on these intervals, such that enlarging the interval by a small amount\n",
    "#yields two instead of three neutrinos\n",
    "\n",
    "##################################\n",
    "#we also tried to multiply the efficiency to the initial matrix, i.e. before inverting but this did not really yield \n",
    "#better results, it worsened the results for tau and muon but made them be less dependend of the cut interval for the\n",
    "#t-s-channel seperation since here the actual electron entries were corrected in the matrix (before inverting), such that \n",
    "#the muon and tuon entries do not get effected this much....\n",
    "\n",
    "######\n",
    "#systematic errors: by cutting of an interval for s-t-channel, interference terms are neglected, this might lead to some\n",
    "#deviation\n",
    "#moreover half of the electrons are not measured correctly, this just leads to smaller number of overall electrons but\n",
    "#well it worsens the cross-section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both plots from above next to each other for protocoll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 5))\n",
    "plt.suptitle('Selection of the t- and s- Channel Contributions',y=1.07, size=19)\n",
    "fig.text(0.55, 0, r'Angle in $\\cos{\\theta}$', ha='center', fontsize=15)\n",
    "fig.text(0.11, 0.5, r'Number of Particles per Bin', va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "\n",
    "plt.subplot(121)\n",
    "entries1, bin_edges1, patches1=plt.hist(criteria[1][4], bins=500,range=(-1,1),alpha=0.5)\n",
    "x=np.linspace(-0.99,0.99,len(entries1))\n",
    "    \n",
    "bin_middles1 = 0.5 * (bin_edges1[1:] + bin_edges1[:-1])\n",
    "popt1, pcov1= curve_fit(f,bin_middles1[17:-20],entries1[17:-20],sigma=np.sqrt(entries1[17:-20]),p0=[60.7,7.2], maxfev=2000)\n",
    "plt.plot(x, f(x, *popt1), label='total')\n",
    "plt.plot(x, popt1[0]*(1+x**2), label='s-channel')\n",
    "plt.plot(x, popt1[1]/(1-x)**2, label='t-channel')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title('Monte Carlo Simulation Data', fontsize=16)\n",
    "plt.xlim(-1.1,1.1)\n",
    "\n",
    "         \n",
    "         \n",
    "redchi1=redchi2(entries1[17:-20],f(bin_middles1[17:-20],*popt1),np.sqrt(entries1[17:-20]),2)\n",
    "\n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    r'$s=(%.1f \\pm %.1f) $' % (popt1[0],np.sqrt(pcov1[0][0]), ),\n",
    "    r'$t=(%.1f \\pm %.1f) $ ' % (popt1[1],np.sqrt(pcov1[1][1]), ),\n",
    "    r'$\\chi^2_{red}=%.1f$' % (redchi1, )))\n",
    "\n",
    "plt.text(0, 600, textstr, size=15,ha=\"center\",\n",
    "         va=\"center\",bbox=dict(boxstyle=\"round\",facecolor='lightblue', edgecolor='darkblue', ), color='black')\n",
    "\n",
    "plt.ylim(0,1000)\n",
    "plt.subplot(122)\n",
    "bins=200\n",
    "entries, bin_edges, patches=plt.hist(cos_thet_ee, bins=bins,range=(-1,1),alpha=0.5)\n",
    "entries2, bin_edges2, patches=plt.hist(cos_thet_s_channel, bins=200,range=(-1,1),alpha=0.5, color='red')\n",
    "x=np.linspace(-0.99,0.99,len(entries))\n",
    "    \n",
    "bin_middles = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "bin_middles2 = 0.5 * (bin_edges2[1:] + bin_edges2[:-1])\n",
    "\n",
    "popt, pcov= curve_fit(f,bin_middles[17:-20],entries[17:-20],sigma=np.sqrt(entries[17:-20]),p0=[60.7,7.2], maxfev=2000)\n",
    "plt.plot(x, f(x, *popt), label='total')\n",
    "plt.plot(x, popt[0]*(1+x**2), label='s-channel')\n",
    "plt.plot(x, popt[1]/(1-x)**2, label='t-channel')         \n",
    "         \n",
    "\n",
    "redchi=redchi2(entries[17:-20],f(bin_middles[17:-20],*popt),np.sqrt(entries[17:-20]))\n",
    "\n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    r'$s=(%.1f \\pm %.1f) $' % (popt[0],np.sqrt(pcov[0][0]), ),\n",
    "    r'$t=(%.1f \\pm %.1f) $ ' % (popt[1],np.sqrt(pcov[1][1]), ),\n",
    "    r'$\\chi^2_{red}=%.1f$' % (redchi, )))\n",
    "\n",
    "plt.text(0, 200, textstr, size=15,ha=\"center\",\n",
    "         va=\"center\",bbox=dict(boxstyle=\"round\",facecolor='lightblue', edgecolor='darkblue', ), color='black')\n",
    "\n",
    "\n",
    "plt.title('Real OPAL Data', fontsize=16)\n",
    "plt.xlim(-1.1,1.1)\n",
    "plt.ylim(0,270)\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Measurement of the total production cross sections\n",
    "\n",
    "For **each** of the seven centre-of-mass energies:\n",
    "* Determine the number of events in the handronic channel *and* in the three leptonic channels\n",
    "* Substract the background and correct for selection efficiencies accordingly\n",
    "* Then, calculate the differnetial cross sections for the hadronic *and* the leptnic channels\n",
    "* Add the radiation corrections from The table given below. **Don't forget to take the uncertainties (errors) into account!**\n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Correction hadronic channel    \\[nb\\] |  Correction leptonic channel   \\[nb\\]|\n",
    "| --- | --- | --- |\n",
    "| 88.47 | +2.0  | +0.09 |\n",
    "| 89.46 | +4.3  | +0.20 |\n",
    "| 90.22 | +7.7  | +0.36 |\n",
    "| 91.22 | +10.8 | +0.52 |\n",
    "| 91.97 | +4.7  | +0.22 |\n",
    "| 92.96 | -0.2  | -0.01 |\n",
    "| 93.76 | -1.6  | -0.08 |\n",
    "\n",
    "Feel free to access these values using the dictionary 'xs_corrections' given below.\n",
    "* Once the total cross section for all four decay channels at all seven energies have been measured, fit a **Breit-Wigner distribution** to measure the $Z$ boson mass ($m_Z$) and the resonance width ($\\Gamma_Z$) and the peak cross section s of the resonance for the hadronic and the leptonic channels. Again, **propagate the uncertainties carefully**.\n",
    "* Compare your results to the OPAL cross section s and the theoretical predictions. How many degrees of freedom does the fit have? How can you udge if the model is compatible with the measured data? Calculate the  **confidence levels**.\n",
    "* Calculate the partial widths for all channels from the measured cross sections on the peak. Which is the best partial width to start with? Compare them with the theoretical predictions and the values that you have calculated in the beginning.\n",
    "* Determine from your results the **number of generations of light neutrinos**. Which assumptions are necessary?\n",
    "* Discuss in detail the systematic uncertainties in the whole procedure of the analysis. Which assumptions were necessary?\n",
    "\n",
    "These are some **references** that might be interesting to look up:\n",
    "* Particle Data Book: https://pdg.lbl.gov/2020/download/Prog.Theor.Exp.Phys.2020.083C01.pdf\n",
    "** Resonances: https://pdg.lbl.gov/2017/reviews/rpp2017-rev-resonances.pdf\n",
    "* Precision Electroweak Measurements on the Z Resonance (Combination LEP): https://arxiv.org/abs/hep-ex/0509008\n",
    "* Measurement of the $Z^0$ mass and width with the OPAL detector at LEP: https://doi.org/10.1016/0370-2693(89)90705-3\n",
    "* Measurement of the $Z^0$ line shape parameters and the electroweak couplings of charged leptons: https://inspirehep.net/literature/315269\n",
    "* The OPAL Collaboration, *Precise Determination of the $Z$ Resonance Parameters at LEP: \"Zedometry\"*: https://arxiv.org/abs/hep-ex/0012018\n",
    "* Fitting a Breit-Wigner curve using uproot: https://masonproffitt.github.io/uproot-tutorial/07-fitting/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrections for the single channels and the mean_enegy, luminosity from the csv file\n",
    "corr_hadr=[2.0, 4.3, 7.7, 10.8, 4.7, -0.2, -1.6]\n",
    "corr_lep=[0.09, 0.20, 0.36, 0.52, 0.22, -0.01, -0.08]\n",
    "mean_energy, lumi,lumi_stat, lumi_sys, lumi_alll=CSV('daten_2.csv')\n",
    "\n",
    "#lit value for Mz\n",
    "Mz_lit =91.1876\n",
    "dMz_lit=0.0021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadron1=[]\n",
    "ele1=[]\n",
    "muon1=[]\n",
    "tau1=[]\n",
    "dhadron1=[]\n",
    "dele1=[]\n",
    "dmuon1=[]\n",
    "dtau1=[]\n",
    "dhadron1_sys=[]\n",
    "dele1_sys=[]\n",
    "dmuon1_sys=[]\n",
    "dtau1_sys=[]\n",
    "hadron2=[]\n",
    "ele2=[]\n",
    "muon2=[]\n",
    "tau2=[]\n",
    "#cos_thet_e=[]\n",
    "cos_thet_muon=[]\n",
    "e_s_channel=[]\n",
    "\n",
    "#defining a vector for the correction of the electron number\n",
    "#to pick out s-channel electrons and correcting for interference part electrons\n",
    "s_channel_vec=np.array([1,ratio*efficiency_clean3, 1,1])\n",
    "ds_channel_vec=np.array([0,ratio*np.sqrt((defficiency_clean3)**2),0,0])\n",
    "\n",
    "#matrix3=np.copy(matrix)\n",
    "#matrix3[1]=matrix[1]*efficiency_clean3\n",
    "#inverse_matrix1=np.linalg.inv(matrix)\n",
    "\n",
    "\n",
    "for i in range(0, 7):\n",
    "\n",
    "    num1, ph1, cos_thet_muon_i, e_s_channel_i=(count_particles_simulated(criteria2[i][0],criteria2[i][1],criteria2[i][2],criteria2[i][3],criteria2[i][4]))\n",
    "    \n",
    "    e_s_channel.append(e_s_channel_i)\n",
    "    \n",
    "    #replace number total electron number by number of s-channel electrons\n",
    "    num1[1]=len(e_s_channel[i])\n",
    "    num1=np.array(num1)\n",
    "\n",
    "    #creating a list for the costheta of muons for the calculation of the asymmetry factor and the Weinber angle\n",
    "    cos_thet_muon.append(cos_thet_muon_i)\n",
    "    \n",
    "    #using the second counting function\n",
    "    num2=(particletest(criteria2[i][1],criteria2[i][0],criteria2[i][2],criteria2[i][3]))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    #Define vector containing corrections on cross section for different channels\n",
    "    corr_vec=np.array([corr_hadr[i], corr_lep[i],corr_lep[i], corr_lep[i]])\n",
    "    \n",
    "    #multiplying inverse matrix to number vector\n",
    "    #correcting for s-channel electrons \n",
    "    #deviding ny the luminosities for the respective mean energy\n",
    "    #adding corrections on single channels\n",
    "    \n",
    "    sigma_clean1=np.dot(inverse_matrix1,num1)*s_channel_vec/lumi[i]+corr_vec\n",
    "    \n",
    "    #statistical error on cross section\n",
    "    dsigma_clean1=np.sqrt((np.dot(error_matrix**2,num1**2)*s_channel_vec**2/lumi[i]**2)\n",
    "        +(np.dot(inverse_matrix1**2,np.sqrt(num1)**2)*s_channel_vec**2/lumi[i]**2)\n",
    "        +(np.dot(inverse_matrix1**2,num1**2)*ds_channel_vec**2/lumi[i]**2)\n",
    "        +(lumi_stat[i]**2*np.dot(inverse_matrix1**2,num1**2)*s_channel_vec**2/lumi[i]**4))\n",
    "    \n",
    "    #systematic error on cross section\n",
    "    dsigma_clean1_sys=np.sqrt(np.abs((lumi_sys[i]*np.dot(inverse_matrix1**2,num1**2)))*s_channel_vec**2/lumi[i]**4)\n",
    "    \n",
    "    \n",
    "    #cross sections obtained by using counting function 2\n",
    "    sigma_clean2=np.dot(inverse_matrix2,np.array(num2))*s_channel_vec/lumi[i]+corr_vec\n",
    "\n",
    "    #create lists for all particles containing the seven cross sections for the seven mean energies\n",
    "    \n",
    "    hadron1.append(sigma_clean1[0])\n",
    "    ele1.append(sigma_clean1[1])\n",
    "    muon1.append(sigma_clean1[2])\n",
    "    tau1.append(sigma_clean1[3])\n",
    "    dhadron1.append(dsigma_clean1[0])\n",
    "    dele1.append(dsigma_clean1[1])\n",
    "    dmuon1.append(dsigma_clean1[2])\n",
    "    dtau1.append(dsigma_clean1[3])\n",
    "    dhadron1_sys.append(dsigma_clean1_sys[0])\n",
    "    dele1_sys.append(dsigma_clean1_sys[1])\n",
    "    dmuon1_sys.append(dsigma_clean1_sys[2])\n",
    "    dtau1_sys.append(dsigma_clean1_sys[3])\n",
    "    hadron2.append(sigma_clean2[0])\n",
    "    ele2.append(sigma_clean2[1])\n",
    "    muon2.append(sigma_clean2[2])\n",
    "    tau2.append(sigma_clean2[3])\n",
    "\n",
    "#adding all particle's cross section to one list\n",
    "cross_section1=np.array([hadron1, ele1,tau1,muon1])  \n",
    "dcross_section1=np.array([dhadron1, dele1,dtau1,dmuon1])  \n",
    "dcross_section1_sys=np.array([dhadron1_sys, dele1_sys,dtau1_sys,dmuon1_sys])  \n",
    "cross_section2=np.array([hadron2, ele2,tau2,muon2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a factor to transform cross section's unit from nb to 1/GeV^2\n",
    "factor=1/(1.973e-16)**2*1e-37\n",
    "\n",
    "Mz=[]\n",
    "dMz=[]\n",
    "gamma_fe=[]\n",
    "dgamma_fe=[]\n",
    "gamma_z=[]\n",
    "dgamma_z=[]\n",
    "sigma_max=[]\n",
    "dsigma_max=[]\n",
    "\n",
    "\n",
    "\n",
    "#plotting the cross secion vs mean energy once in nb and once in 1/GeV^2\n",
    "cross_sec=[cross_section1, cross_section1*factor]\n",
    "dcross_sec=[dcross_section1,dcross_section1*factor ]\n",
    "labels=['Cross-sections for Hadrons','Cross-sections for Electrons','Cross-sections for Tau', 'Cross-sections for Myons']\n",
    "xlabel=[r'[nb]', r'[\\left(1/GeV\\right)^2]']\n",
    "unit=[r'\\left(GeV\\right)^4 \\mu b', r' \\cdot 10^3\\ \\left( GeV\\right)^2' ]\n",
    "a= [1/1000,1e3]\n",
    "b=[0,1]\n",
    "\n",
    "\n",
    "\n",
    "for u in [0,1]:#plotting the cross secion vs mean energy once in nb and once in 1/GeV^2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (15, 8))\n",
    "\n",
    "    plt.suptitle('Total Cross-sections as Functions of the Total Center of Mass Energy',y=1.03, size=17)\n",
    "\n",
    "    fig.text(0.55, 0.1, r'Center of Mass Energy [GeV]', ha='center', fontsize=14)\n",
    "    fig.text(0.1025, 0.5, r'Cross-section {}'.format(xlabel[u]), va='center', rotation='vertical', fontsize=15)\n",
    "\n",
    "    for i in [0,1,2,3]:  #for all particle types\n",
    "        plt.subplot(221+i)\n",
    "        plt.title(labels[i], fontsize=13)\n",
    "        plt.errorbar(mean_energy, cross_sec[u][i],dcross_sec[u][i],0, '*', color='darkblue', capsize=4,label=labels[i])\n",
    "        popt1, pcov1=curve_fit(breit_wigner, mean_energy, cross_sec[u][i],sigma=dcross_sec[u][i], absolute_sigma=True, p0=[-91.18501194,252.75591492*230.92674034,2.54017013])\n",
    "        #popt2, pcov2=curve_fit(breit_wigner, mean_energy, cross_section2[i])\n",
    "        x=np.linspace(86, 95, 500)\n",
    "        plt.xlim(84,95)\n",
    "        plt.ylim(0,np.max(cross_sec[u][i])*1.12)\n",
    "        plt.plot(x, breit_wigner(x, *popt1), color='lightblue')\n",
    "        #plt.plot(x, breit_wigner(x, *popt2),color='pink')\n",
    "        plt.grid()\n",
    "        #plt.legend()\n",
    "\n",
    "        #number of parameter: 3\n",
    "        redchi=redchi2(cross_sec[u][i],breit_wigner(mean_energy,*popt1), dcross_sec[u][i],3)\n",
    "\n",
    "\n",
    "        textstr = '\\n'.join((\n",
    "            r'$M_Z=(%.2f \\pm %.2f) $GeV' % (np.abs(popt1[0]),np.sqrt(pcov1[0][0]), ),\n",
    "            r'$\\Gamma_e \\Gamma_f=(%.{}f \\pm %.{}f) {}$ '.format(b[u],b[u],unit[u]) % (popt1[1]*a[u],np.sqrt(pcov1[1][1])*a[u], ),\n",
    "            r'$\\Gamma_Z=(%.2f \\pm %.2f) $GeV ' % (popt1[2],np.sqrt(pcov1[2][2]), ),\n",
    "            r'$\\chi^2_{red}=%.1f$' % (redchi, )))\n",
    "\n",
    "        plt.text(mean_energy[0]-1.65, cross_sec[u][i][2]*1.16, textstr, size=14,ha=\"center\",\n",
    "                 va=\"center\",bbox=dict(boxstyle=\"round\",facecolor='lightblue', edgecolor='darkblue', ), color='black')\n",
    "\n",
    "\n",
    "        #inserting parameters into lists\n",
    "        if u ==1:\n",
    "            Mz.append(np.abs(popt1[0]))\n",
    "            dMz.append(np.sqrt(np.abs(pcov1[0][0])))\n",
    "            gamma_fe.append(np.abs(popt1[1]))\n",
    "            dgamma_fe.append(np.sqrt(np.abs(pcov1[1][1])))\n",
    "            gamma_z.append(np.abs(popt1[2]))\n",
    "            dgamma_z.append(np.sqrt(np.abs(pcov1[2][2])))\n",
    "            sigma_max.append(breit_wigner(popt1[0], *popt1))\n",
    "            dsigma_max.append(12*np.pi*np.sqrt(pcov1[1][1]/(popt1[0]**2*popt1[2]**2)**2\n",
    "                                +4*popt1[1]**2*pcov1[0][0]/(popt1[0]**3*popt1[2]**2)**2\n",
    "                                               +4*popt1[1]**2*pcov1[2][2]/(popt1[0]**2*popt1[2]**3)**2))\n",
    "            #plt.plot(x, breit_wigner(x, 91.182,0.0838**2, 2.54), 'red')\n",
    "\n",
    "            \n",
    "#the systematic error on the cross section is not used since its direction is not known\n",
    "#thus we dont know if it should be added or subtracted.\n",
    "#Moreover the systematic error turns out to be three magnitudes smaller than the value to be corrected and one magnitude\n",
    "#smaller than the statistical error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing both counting methods\n",
    "\n",
    "#plotting the cross secion vs mean energy once in nb and once in 1/GeV^2\n",
    "cross_sec=[cross_section1, cross_section1*factor]\n",
    "dcross_sec=[dcross_section1,dcross_section1*factor ]\n",
    "labels=['Cross-sections for Hadrons','Cross-sections for Electrons','Cross-sections for Tau', 'Cross-sections for Myons']\n",
    "xlabel=[r'[nb]', r'[\\left(1/GeV\\right)^2]']\n",
    "unit=[r'\\left(GeV\\right)^4 \\mu b', r' \\cdot 10^3\\ \\left( GeV\\right)^2' ]\n",
    "a= [1/1000,1e3]\n",
    "b=[0,1]\n",
    "\n",
    "\n",
    "\n",
    "for u in [1]:#plotting the cross secion vs mean energy once in nb and once in 1/GeV^2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (15, 8))\n",
    "\n",
    "    plt.suptitle('Total Cross-sections as Functions of the Total Center of Mass Energy',y=1.03, size=17)\n",
    "\n",
    "    fig.text(0.55, 0.1, r'Center of Mass Energy [GeV]', ha='center', fontsize=14)\n",
    "    fig.text(0.1025, 0.5, r'Cross-section {}'.format(xlabel[u]), va='center', rotation='vertical', fontsize=15)\n",
    "    a=[1.73,0.0838,0.0838,0.0838]\n",
    "    for i in [0,1,2,3]:  #for all particle types\n",
    "        plt.subplot(221+i)\n",
    "        plt.title(labels[i], fontsize=13)\n",
    "        plt.errorbar(mean_energy, cross_sec[u][i],dcross_sec[u][i],0, '*', color='darkblue', capsize=4,label='counting method 1')\n",
    "        \n",
    "        \n",
    "        plt.plot(mean_energy, cross_section2[i]*factor,'*', color='red', label='counting method 2') #counting method 2\n",
    "        popt1, pcov1=curve_fit(breit_wigner, mean_energy, cross_sec[u][i],sigma=dcross_sec[u][i], absolute_sigma=True, p0=[-91.18501194,252.75591492*230.92674034,2.54017013])\n",
    "       \n",
    "        popt2, pcov2=curve_fit(breit_wigner, mean_energy, cross_section2[i]*factor,p0=[-91.18501194,252.75591492*230.92674034,2.54017013])\n",
    "        plt.plot(x, breit_wigner(x, 91.182,0.0838*a[i], 2.54), color= 'green', label='expected')\n",
    "        x=np.linspace(86, 95, 500)\n",
    "        plt.xlim(84,95)\n",
    "        plt.ylim(0,np.max(cross_sec[u][i])*1.12)\n",
    "        plt.plot(x, breit_wigner(x, *popt1), color='lightblue')\n",
    "        plt.plot(x, breit_wigner(x, *popt2),color='pink')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "\n",
    "#interpretation:\n",
    "#there must be some systematic error on the electrons' cross section since the gauss function is shifted to the right\n",
    "#such that the Z boson mass seems to be lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying some results and ttest\n",
    "\n",
    "gamma_e=np.sqrt(gamma_fe[1])\n",
    "dgamma_e=dgamma_fe[1]/(2*gamma_e)\n",
    "gammas=np.array([])\n",
    "dgammas=np.array([])\n",
    "for i in [0,1,2,3]:\n",
    "    gammas=np.append(gammas, gamma_fe[i]/gamma_e)\n",
    "    dgammas=np.append(dgammas, np.sqrt(dgamma_fe[i]**2/gamma_e**2+dgamma_e**2*gamma_fe[i]**2/gamma_e**4))\n",
    "gammas[1]=gamma_e\n",
    "dgammas[1]=dgamma_e\n",
    "hadr_lit=(2*0.299+3*0.378)\n",
    "lep_lit=83.8\n",
    "print('partial decay width:')\n",
    "print('-----------------------------------------------------')\n",
    "print('hadrons:   literature value: %.2f GeV' %(hadr_lit))\n",
    "print(r'measured:( %.2f  +/- %.2f) GeV, ttest:  %.1f' %(gammas[0],dgammas[0],ttest(gammas[0], dgammas[0], hadr_lit)))\n",
    "print('leptons:   literature value: %.1f MeV'%(lep_lit))\n",
    "print('electrons:( %.0f  +/- %.0f) MeV, ttest:  %.1f' %(gammas[1]*1000,dgammas[1]*1000,ttest(gammas[1]*1000, dgammas[1]*1000, lep_lit)))\n",
    "print('taus:( %.0f  +/- %.0f) MeV, ttest:  %.1f' %(gammas[2]*1000,dgammas[2]*1000,ttest(gammas[2]*1000, dgammas[2]*1000, lep_lit)))\n",
    "print('muons:( %.0f  +/- %.0f) MeV, ttest:  %.1f' %(gammas[3]*1000,dgammas[3]*1000,ttest(gammas[3]*1000, dgammas[3]*1000, lep_lit)))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "\n",
    "sigma_max=np.array(sigma_max)\n",
    "dsigma_max=np.array(dsigma_max)\n",
    "gamma_z=np.array(gamma_z)\n",
    "dgamma_z=np.array(dgamma_z)\n",
    "gamma_z_weigh=np.sum(gamma_z*sigma_max)/np.sum(sigma_max)\n",
    "dgamma_z_weigh=np.sqrt((np.sum(dgamma_z**2*sigma_max**2)+np.sum(gamma_z**2*dsigma_max**2))/np.sum(sigma_max)**2\n",
    "                       +np.sum(gamma_z**2*sigma_max**2)*np.sum(dsigma_max**2)/np.sum(sigma_max)**4)\n",
    "print('weighted total decay width: (%.1f +/- %.1f) GeV'%(gamma_z_weigh,dgamma_z_weigh))\n",
    "print('total decay width obtained by hadrons: (%.2f +/- %.2f) GeV'%(gamma_z[0],dgamma_z[0]))\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "#assumption made here: \n",
    "#the Standard Model is correct and there are no other particles than the one described within this theory\n",
    "\n",
    "gamma_nu = 167.6/1000\n",
    "gamma_nu_ges=gamma_z_weigh-np.sum(gammas)\n",
    "dgamma_nu_ges=np.sqrt(dgamma_z_weigh**2+np.sum(dgammas**2))\n",
    "print('partial width neutrinos, %.1f MeV' %(gamma_nu*1000))\n",
    "print('measured: (%.1f +/- %.1f) MeV' %(gamma_nu_ges*1000/3, dgamma_nu_ges*1000/3))\n",
    "number_nu=gamma_nu_ges/gamma_nu\n",
    "dnumber_nu=np.sqrt(dgamma_nu_ges**2/gamma_nu**2)\n",
    "print('number of lepton generations: (%.1f +/- %.1f) '%(number_nu, dnumber_nu))\n",
    "\n",
    "print('-------------------------------------------------------')\n",
    "print('using total decay width obtained by hadrons')\n",
    "gamma_nu_ges=gamma_z[0]-np.sum(gammas)\n",
    "dgamma_nu_ges=np.sqrt(dgamma_z[0]**2+np.sum(dgammas**2))\n",
    "print('partial decay width neutrinos, %.1f' %(gamma_nu*1000))\n",
    "print('measured: (%.1f +/- %.1f) MeV' %(gamma_nu_ges*1000/3, dgamma_nu_ges*1000/3))\n",
    "number_nu=gamma_nu_ges/gamma_nu\n",
    "dnumber_nu=np.sqrt(dgamma_nu_ges**2/gamma_nu**2)\n",
    "print('number of lepton generations: (%.1f +/- %.1f) '%(number_nu, dnumber_nu))\n",
    "print('ttest lepton generations: %.1f' %(ttest(number_nu, dnumber_nu, 3))) #comparison with number of lepton generations as we know it so far\n",
    "print('---------------------------------------------------------')\n",
    "print('Mean of measured Z boson masses')\n",
    "Mz_mean=np.mean(Mz)\n",
    "dMz_mean=np.std(Mz)\n",
    "dMz_mean_2=np.sum(np.array(dMz)**2)/2\n",
    "print('Mz=(%.2f +/- %.2f)GeV' %(Mz_mean, dMz_mean))\n",
    "print('ttest= %.1f'%(ttest(Mz_mean, dMz_mean, Mz_lit, dMz_lit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Forward-backward asymmetry and $\\sin^2(\\theta_\\text{W})$ in muon final states\n",
    "\n",
    "* Using the **muon channel only**, measure the forward-backward asymmetry $\\mathcal{A}_\\text{FB}$ using OPAL data and muon Monte Carlo events. Take into account the radiation corrections given below. \n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Radiation correction [-]|  \n",
    "| --- | --- | \n",
    "| 88.47 | 0.021512  | \n",
    "| 89.46 | 0.019262  | \n",
    "| 90.22 | 0.016713  | \n",
    "| 91.22 | 0.018293  | \n",
    "| 91.97 | 0.030286  | \n",
    "| 92.96 | 0.062196  | \n",
    "| 93.76 | 0.093850  | \n",
    "\n",
    "Feel free to use the dictionary 'radiation_corrections' given below.\n",
    "\n",
    "* Measure the **Weinberg angle** as $\\sin^2(\\theta_\\text{W})$. Compare the measurement with the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correction=[0.021512, 0.019262, 0.016713, 0.018293, 0.030286, 0.062196, 0.093850]\n",
    "\n",
    "#literature value for sintheta^2\n",
    "angle_lit=0.23122\n",
    "dangle_lit=0.00004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(r'$cos \\theta $ values for simulated muons')\n",
    "\n",
    "\n",
    "#create histogram\n",
    "entries, bin_edges, patches=plt.hist(criteria[3][4], bins=100,range=(-1,1), label='muon',alpha=0.5)\n",
    "x=np.linspace(-0.99,0.99,len(entries))\n",
    "    \n",
    "bin_middles = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "          \n",
    "z=50\n",
    "plt.axvline(bin_middles[z], color='darkblue')\n",
    "\n",
    "#count muons on left and right side\n",
    "N_b = sum(entries[:z])\n",
    "N_f=sum(entries[z:])\n",
    "         \n",
    "#checking the mean energy of the simulated data to know which correction to use\n",
    "print('mean energy:%.3f GeV'%(criteria[3][5][0]*2))\n",
    "          \n",
    "          \n",
    "Asym = (N_f-N_b)/(N_f+N_b) +correction[3]\n",
    "d_Asym= np.sqrt(N_f/(N_f+N_b)**2+ ((N_f-N_b)/(N_f+N_b)**2)**2*N_f+N_b/(N_f+N_b)**2+ ((N_f-N_b)/(N_f+N_b)**2)**2*N_b)\n",
    "\n",
    "\n",
    "sintheta2=1/4*(1-np.sqrt(Asym/3)) #approximation used since mean energy close to resonance energy\n",
    "\n",
    "dsintheta2=1/8/np.sqrt(Asym/3)*1/3*np.sqrt(d_Asym) \n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    r'$A_{FB}=(%.3f \\pm %.3f) $' % (Asym, d_Asym, ),\n",
    "    r'$sin\\theta^2=(%.2f \\pm %.2f) $ ' % (sintheta2,dsintheta2, ),\n",
    "    r'$ttest=%.1f$' % (ttest(sintheta2,dsintheta2,angle_lit, dangle_lit), )))\n",
    "\n",
    "plt.text(-0.6, 800, textstr, size=15,ha=\"center\",\n",
    "         va=\"center\",bbox=dict(boxstyle=\"round\",facecolor='lightblue', edgecolor='darkblue', ), color='black')\n",
    "plt.xlabel(r'angle of detection $\\cos{\\theta}$', fontsize=15)\n",
    "plt.xlabel(r'number of particles per bin', fontsize=15)\n",
    "\n",
    "#plt.plot(x, f(x,6,7), color='red')\n",
    "    \n",
    "plt.xlim(-1.1,1.1)\n",
    "#plt.ylim(0,900)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPAL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 10))\n",
    "plt.suptitle(r'$cos \\theta $ values for measured muons', fontsize=20)\n",
    "fig.text(0.5, 0.1,r'angle of detection $\\cos{\\theta}$', va='center', rotation='horizontal', fontsize=20)\n",
    "fig.text(0.11, 0.5, r'number of particles per bin', va='center', rotation='vertical', fontsize=20)\n",
    "bins=50\n",
    "Asym=[]\n",
    "dAsym=[]\n",
    "for i in range(0,7):\n",
    "    plt.subplot(331+i)\n",
    "    entries, bin_edges, patches=plt.hist(cos_thet_muon[i], bins=bins,range=(-1,1), label='E=%.2f GeV'%(mean_energy[i]),alpha=0.5)\n",
    "    x=np.linspace(-0.99,0.99,len(entries))\n",
    "\n",
    "    bin_middles = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "    z=int(bins/2)   \n",
    "    plt.axvline(bin_middles[z], color='red')\n",
    "    N_b = sum(entries[:z])\n",
    "    N_f=sum(entries[z:])\n",
    "    Asymi=(N_f-N_b)/(N_f+N_b)+correction[i]\n",
    "    Asym.append(Asymi)\n",
    "    d_Asymi=np.sqrt(N_f/(N_f+N_b)**2+ ((N_f-N_b)/(N_f+N_b)**2)**2*N_f+N_b/(N_f+N_b)**2\n",
    "                          + ((N_f-N_b)/(N_f+N_b)**2)**2*N_b)\n",
    "    dAsym.append(d_Asymi)\n",
    "\n",
    "    \n",
    "    sintheta2=1/4*(1-np.sqrt(np.abs(Asymi)/3))\n",
    "\n",
    "    dsintheta2=1/8/np.sqrt(np.abs(Asymi)/3)*1/3*np.sqrt(d_Asymi)  \n",
    "\n",
    "    textstr = '\\n'.join((\n",
    "        r'$A_{FB}=(%.2f \\pm %.2f) $' % (Asymi, d_Asymi, ),\n",
    "        r'$sin\\theta^2=(%.2f \\pm %.2f) $ ' % (sintheta2,dsintheta2, ), #approximation only reasonable at resonace energy\n",
    "        r'$ttest=%.1f$' % (ttest(sintheta2,dsintheta2,angle_lit, dangle_lit), )))\n",
    "\n",
    "    plt.text(-0.5, max(entries)*1.01, textstr, size=15,ha=\"center\",\n",
    "             va=\"center\",bbox=dict(boxstyle=\"round\",facecolor='lightblue', edgecolor='darkblue', ), color='black')\n",
    "\n",
    "\n",
    "    plt.xlim(-1,1)\n",
    "    \n",
    "    \n",
    "    plt.ylim(0,max(entries)*1.5)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot asym_corr vs. mean energy\n",
    "#and linear fit\n",
    "\n",
    "#in order to use the following approximation the Asymmetry factor has to be measured exactly at the resonance energy \n",
    "#therefore the asymmetry factors of all energies are plotted and fitted with a linear function\n",
    "#by doing this the asymmetry factor at the resonance energy can be calculated\n",
    "\n",
    "\n",
    "plt.title('The Forward Backward Asymmetry Factor in Dependence of the COM Energy', fontsize=15, y=1.05)\n",
    "Asym_corr=(np.array(Asym))\n",
    "plt.xlabel('Center of mass energy in [GeV]', fontsize=13)\n",
    "plt.ylabel('Forward backward asymmetry factor', fontsize=13)\n",
    "plt.errorbar(mean_energy, Asym_corr,dAsym,0,'*')\n",
    "\n",
    "popt_lin, pcov_lin=curve_fit(lin, mean_energy,Asym_corr, sigma=dAsym, absolute_sigma=True)\n",
    "\n",
    "x=np.linspace(87, 94, 500)\n",
    "plt.plot(x, lin(x, *popt_lin), label='linear fit')\n",
    "\n",
    "redchi_lin=redchi2(Asym_corr, lin(mean_energy, *popt_lin), dAsym)\n",
    "\n",
    "Asym_lin=lin(Mz_lit,*popt_lin)\n",
    "dAsym_lin =np.sqrt(pcov_lin[0][0]*Mz_lit**2+popt[0]**2*dMz_lit**2+pcov_lin[1][1])\n",
    "\n",
    "sintheta2_lin=1/4*(1-np.sqrt(Asym_lin/3))\n",
    "\n",
    "dsintheta2_lin=1/8/np.sqrt(Asym_lin/3)*1/3*dAsym_lin\n",
    "\n",
    "\n",
    "textstr = '\\n'.join((\n",
    "    r'$f(x)=a \\cdot x+b$',\n",
    "r'$a=(%.2f \\pm %.2f) 1/GeV$' % (popt_lin[0],np.sqrt(pcov_lin[0][0]), ),\n",
    "    r'$b=(%.1f \\pm %.1f) $ ' % (popt_lin[1],np.sqrt(pcov_lin[1][1]), ),\n",
    "    r'$\\chi^2_{red}=%.1f$' % (redchi, ),\n",
    "    '$f(91.1876GeV)=(%.3f \\pm %.3f)$' %(Asym_lin,dAsym_lin),\n",
    "    r'$sin \\theta^2 = (%.2f \\pm %.2f)$' %(sintheta2_lin, dsintheta2_lin)))\n",
    "plt.text(89, 0.3, textstr, size=15,ha=\"center\",\n",
    "         va=\"center\",bbox=dict(boxstyle=\"round\",facecolor='lightblue', edgecolor='darkblue', ), color='black')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weinberg angle literature value')\n",
    "print('----------------------------')\n",
    "print('sintheta^2= 0.23122(4)')\n",
    "print('----------------------------')\n",
    "print('Forward-backward asymmetry and Weinberg angle obtained from linear fit:')\n",
    "print('------------------------------------------------')\n",
    "print('A_FB=(%.2f +/- %.2f)' %(Asym_lin, dAsym_lin)) \n",
    "print('sintheta^2= (%.2f +/- %.2f),   ttest: %.1f' %(sintheta2_lin,dsintheta2_lin,ttest(sintheta2_lin,dsintheta2_lin,angle_lit, dangle_lit)))\n",
    "print('------------------------------------------')\n",
    "#since the error is very large due to the errors of the fit\n",
    "#we calculate the asymmetry additionally in the following way:\n",
    "\n",
    "\n",
    "sintheta2_res=1/4*(1-np.sqrt(Asym_corr[3]/3))\n",
    "\n",
    "dsintheta2_res=1/8/np.sqrt(Asym_corr[3]/3)*1/3*np.sqrt(dAsym[3])\n",
    "print('Forward-backward asymmetry and Weinberg angle obtained at given energy close to resonance:')\n",
    "print('------------------------------------------------')\n",
    "print('A_FB=(%.2f +/- %.2f)'%(Asym_corr[3], dAsym[3])) \n",
    "print('sintheta^2= (%.2f +/- %.2f),   ttest: %.1f' %(sintheta2_res,dsintheta2_res,ttest(sintheta2_res,dsintheta2_res,angle_lit, dangle_lit)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Tests on lepton universality¶\n",
    "\n",
    "* Test the lepton universality from the total cross sectinos on the peak for $Z\\to e^+ e^-$, $Z\\to \\mu^+ \\mu^-$ and $Z\\to \\tau^+ \\tau^-$ events. What is the ratio of the total cross section of the hadronic channel to the leptonic channels on the peak? Compare with the ratios obtained from the branching rations and discuss possible differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratio_sigma=sigma_max/sigma_max[0]\n",
    "dratio_sigma=np.sqrt((dsigma_max/sigma_max[0])**2+(sigma_max/sigma_max[0]**2*dsigma_max[0])**2)\n",
    "print('ratio of the hadrons and leptons cross sections:')\n",
    "print('-------------------------------------------------------')\n",
    "print('e/h: (%.0f +/- %.0f) %%'%(ratio_sigma[1]*100, dratio_sigma[1]*100))\n",
    "print('tau/h: (%.1f +/- %.1f) %%'%(ratio_sigma[2]*100, dratio_sigma[2]*100))\n",
    "print('muon/h: (%.1f +/- %.1f)%%'%(ratio_sigma[3]*100, dratio_sigma[3]*100))\n",
    "#ratio_branch= np.mean([gammas[1],gammas[2],gammas[3]])/gammas[0]\n",
    "ratio_branch=gammas/gamma_z_weigh\n",
    "dratio_branch=np.sqrt((dgammas/gamma_z_weigh)**2+(dgamma_z*np.array(gammas)/gamma_z_weigh**2)**2)\n",
    "ratio_ratio=ratio_branch/ratio_branch[0]\n",
    "dratio_ratio=np.sqrt((dratio_branch/ratio_branch[0])**2+(dratio_branch[0]*ratio_branch/ratio_branch[0]**2)**2)\n",
    "print('-------------------------------------------------------')\n",
    "print('branching ratios:')\n",
    "print('-------------------------------------------------------')\n",
    "print('hadrons: (%.1f +/- %.1f) %%'%(ratio_branch[0]*100, dratio_branch[0]*100))\n",
    "print('electrons: (%.1f +/- %.1f) %%'%(ratio_branch[1]*100, dratio_branch[1]*100))\n",
    "print('taus: (%.1f +/- %.1f) %%'%(ratio_branch[2]*100, dratio_branch[2]*100))\n",
    "print('muons: (%.1f +/- %.1f)%%'%(ratio_branch[3]*100, dratio_branch[3]*100))\n",
    "print('-------------------------------------------------------')\n",
    "print('ratios of branching ratios in comparison to ratios of cross sections:')\n",
    "print('-------------------------------------------------------')\n",
    "print('e/h: (%.1f +/- %.1f) %%'%(ratio_ratio[1]*100, dratio_ratio[1]*100),',     ttest: %.1f'% (ttest(ratio_ratio[1], dratio_ratio[1],ratio_sigma[1], dratio_sigma[1]) ))\n",
    "print('tau/h: (%.1f +/- %.1f) %%'%(ratio_ratio[2]*100, dratio_ratio[2]*100),',     ttest: %.1f'% (ttest(ratio_ratio[2], dratio_ratio[2],ratio_sigma[2], dratio_sigma[2]) ))\n",
    "print('muon/h: (%.1f +/- %.1f)%%'%(ratio_ratio[3]*100, dratio_ratio[3]*100),',     ttest: %.1f'% (ttest(ratio_ratio[3], dratio_ratio[3],ratio_sigma[3], dratio_sigma[3]) ))\n",
    "print('-------------------------------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
